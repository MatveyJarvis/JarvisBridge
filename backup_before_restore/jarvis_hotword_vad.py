# jarvis_hotword_vad.py
# Always-on Jarvis: Wake-Word + ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¹ VAD (webrtcvad ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ, Ğ¸Ğ½Ğ°Ñ‡Ğµ energy) + OS-Bridge.
# Ğ—Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸:
#   pip install -U openai python-dotenv sounddevice pydub
#   (Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾) pip install webrtcvad
# ĞÑƒĞ¶ĞµĞ½ ffmpeg Ğ² PATH. ĞŸÑ€Ğ¾Ğ¸Ğ³Ñ€Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ â€” winsound (Windows).

import os, io, time, json, wave, math, collections, re
from dataclasses import dataclass
from typing import List, Tuple, Optional

import sounddevice as sd
from pydub import AudioSegment
from dotenv import load_dotenv
from openai import OpenAI, __version__ as openai_version
import winsound

# --- webrtcvad (ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ) ---
USE_WEBRTCVAD = True
try:
    import webrtcvad  # type: ignore
except Exception:
    USE_WEBRTCVAD = False

# === OS Bridge (Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ) ===
import os_bridge  # Ñ„Ğ°Ğ¹Ğ» os_bridge.py Ñ€ÑĞ´Ğ¾Ğ¼


# ---------------- Config ----------------
def load_config():
    load_dotenv()
    cfg = {
        "OPENAI_API_KEY": os.getenv("OPENAI_API_KEY", "").strip(),
        "OPENAI_PROJECT_ID": os.getenv("OPENAI_PROJECT_ID", "").strip(),

        "OPENAI_MODEL": os.getenv("OPENAI_MODEL", "gpt-4o-mini").strip(),
        "STT_MODEL": os.getenv("STT_MODEL", "gpt-4o-mini-transcribe").strip(),
        "TTS_MODEL": os.getenv("TTS_MODEL", "gpt-4o-mini-tts").strip(),
        "TTS_VOICE": os.getenv("TTS_VOICE", "alloy").strip(),

        "SYSTEM_PROMPT": os.getenv("SYSTEM_PROMPT", (
            "Ğ¢Ñ‹ Jarvis. ĞÑ‚Ğ²ĞµÑ‡Ğ°Ğ¹ ĞºÑ€Ğ°Ñ‚ĞºĞ¾ Ğ¸ Ğ¿Ğ¾ Ğ´ĞµĞ»Ñƒ. "
            "Ğ•ÑĞ»Ğ¸ Ğ½ÑƒĞ¶Ğ½Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ÑŒ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ² Windows, Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚ ÑÑ‚Ñ€Ğ¾ĞºĞ¾Ğ¹:\n"
            "OS: {\"action\":\"open_app|open_path|create_dir|list_dir|search_files\","
            "\"target\":\"...\", \"args\":{}}\n"
            "Ğ¡Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° ÑĞºĞ°Ğ¶Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºÑƒ, Ñ‡Ñ‚Ğ¾ Ğ±ÑƒĞ´ĞµÑˆÑŒ Ğ´ĞµĞ»Ğ°Ñ‚ÑŒ, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾ ÑÑ‚Ñ€Ğ¾ĞºÑƒ OS: {...}."
        )).strip(),

        # Wake word + VAD
        "WAKE_WORD_ENABLED": os.getenv("WAKE_WORD_ENABLED", "true").lower() == "true",
        "WAKE_WORDS": [w.strip() for w in os.getenv(
            "WAKE_WORDS", "Ğ¿Ñ€Ğ¸Ğ²ĞµÑ‚ Ğ´Ğ¶Ğ°Ñ€Ğ²Ğ¸Ñ|ÑĞ¹ Ğ´Ğ¶Ğ°Ñ€Ğ²Ğ¸Ñ|hey jarvis|ok jarvis"
        ).lower().split("|") if w.strip()],

        # VAD Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸
        "VAD_FRAME_MS": int(os.getenv("VAD_FRAME_MS", "20")),          # 10/20/30 (Ğ¿Ğ¾Ğ´ webrtcvad)
        "VAD_MAX_SILENCE_MS": int(os.getenv("VAD_MAX_SILENCE_MS", "1200")),
        "VAD_AGGRESSIVENESS": int(os.getenv("VAD_AGGRESSIVENESS", "2")),  # 0..3 Ğ´Ğ»Ñ webrtcvad
        "VAD_PREROLL_MS": int(os.getenv("VAD_PREROLL_MS", "200")),
        "VAD_POSTROLL_MS": int(os.getenv("VAD_POSTROLL_MS", "300")),

        # Audio (Ğ¶Ñ‘ÑÑ‚ĞºĞ¾ Ğ¿Ğ¾Ğ´ STT): 16k/mono/16bit
        "SAMPLE_RATE": 16000,
        "CHANNELS": 1,

        # UX
        "GREETING_ON_START": os.getenv("GREETING_ON_START", "true").lower() == "true",
        "GREETING_TEXT": os.getenv("GREETING_TEXT", "Ğ“Ğ¾Ñ‚Ğ¾Ğ² Ğº Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ").strip(),
        "LISTENING_BEEP": os.getenv("LISTENING_BEEP", "true").lower() == "true",

        # Ğ¯Ğ·Ñ‹Ğº Ğ´Ğ»Ñ STT
        "STT_LANGUAGE": os.getenv("STT_LANGUAGE", "ru").strip(),
    }
    if not cfg["OPENAI_API_KEY"]:
        raise RuntimeError("OPENAI_API_KEY Ğ¿ÑƒÑÑ‚ Ğ² .env")
    return cfg


def build_openai_client(cfg):
    kwargs = {"api_key": cfg["OPENAI_API_KEY"]}
    if cfg["OPENAI_API_KEY"].startswith("sk-proj-") and cfg["OPENAI_PROJECT_ID"]:
        kwargs["project"] = cfg["OPENAI_PROJECT_ID"]
    return OpenAI(**kwargs)


# ----------- Audio helpers (TTS) -----------
def play_wav(wav_path: str):
    winsound.PlaySound(wav_path, winsound.SND_FILENAME)
    try: os.remove(wav_path)
    except: pass

def bytes_to_wav_in_project(mp3_or_wav_bytes: bytes, name="tts_out") -> str:
    try:
        seg = AudioSegment.from_file(io.BytesIO(mp3_or_wav_bytes), format="mp3")
    except Exception:
        seg = AudioSegment.from_file(io.BytesIO(mp3_or_wav_bytes), format="wav")
    wav_path = os.path.join(os.getcwd(), f"{name}.wav")
    seg.export(wav_path, format="wav", parameters=["-acodec", "pcm_s16le"])
    return wav_path

def tts_say(client: OpenAI, cfg, text: str):
    if not text: return
    mp3_path = os.path.join(os.getcwd(), "tts_out.mp3")
    # streaming
    try:
        with client.audio.speech.with_streaming_response.create(
            model=cfg["TTS_MODEL"], voice=cfg["TTS_VOICE"], input=text
        ) as resp:
            resp.stream_to_file(mp3_path)
        with open(mp3_path, "rb") as f: mp3 = f.read()
        wav = bytes_to_wav_in_project(mp3)
        play_wav(wav)
        try: os.remove(mp3_path)
        except: pass
        return
    except Exception:
        pass
    # fallback
    speech = client.audio.speech.create(model=cfg["TTS_MODEL"], voice=cfg["TTS_VOICE"], input=text)
    audio_bytes = speech.read()
    wav = bytes_to_wav_in_project(audio_bytes)
    play_wav(wav)


# ----------- VAD recorder (ring buffer) -----------
@dataclass
class VadConfig:
    samplerate: int = 16000
    frame_ms: int = 20
    max_silence_ms: int = 1200
    aggressiveness: int = 2
    preroll_ms: int = 200
    postroll_ms: int = 300

def _rms_int16(pcm: bytes) -> float:
    n = len(pcm) // 2
    if n == 0: return 0.0
    s = 0
    for i in range(0, len(pcm), 2):
        v = int.from_bytes(pcm[i:i+2], "little", signed=True)
        s += v * v
    return math.sqrt(s / n)

def record_utterance_vad(vad_cfg: VadConfig) -> bytes:
    """
    Ğ—Ğ°Ğ¿Ğ¸ÑÑŒ Ğ²Ñ‹ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´/Ğ¿Ğ¾ÑÑ‚-Ğ±ÑƒÑ„ĞµÑ€Ğ¾Ğ¼:
    - RawInputStream 16k/mono/16bit
    - ring-buffer Ğ½Ğ° preroll/postroll
    - ÑÑ‚Ğ°Ñ€Ñ‚ Ğ¿Ğ¾ Ñ€ĞµÑ‡Ğ¸, ÑÑ‚Ğ¾Ğ¿ Ğ¿Ğ¾ Ñ‚Ğ¸ÑˆĞ¸Ğ½Ğµ max_silence_ms
    - Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğ¹ WAV
    """
    bytes_per_frame = int(vad_cfg.samplerate * vad_cfg.frame_ms / 1000) * 2  # 16-bit mono
    pre_frames = max(1, vad_cfg.preroll_ms // vad_cfg.frame_ms)
    post_frames = max(1, vad_cfg.postroll_ms // vad_cfg.frame_ms)
    ring = collections.deque(maxlen=pre_frames)

    vad = webrtcvad.Vad(vad_cfg.aggressiveness) if USE_WEBRTCVAD else None

    frames: List[bytes] = []
    silence_ms = 0
    speaking = False

    with sd.RawInputStream(samplerate=vad_cfg.samplerate, channels=1, dtype="int16") as stream:
        # ĞšĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ° Ğ´Ğ»Ñ energy-VAD
        noise_vals = []
        for _ in range(max(1, int(400 / vad_cfg.frame_ms))):
            buf = stream.read(bytes_per_frame)[0]
            noise_vals.append(_rms_int16(buf))
        noise_floor = (sum(noise_vals) / len(noise_vals)) if noise_vals else 200.0
        start_thr = max(250.0, noise_floor * 1.5)
        stop_thr  = max(180.0, noise_floor * 1.2)

        # ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ñ†Ğ¸ĞºĞ»
        while True:
            buf = stream.read(bytes_per_frame)[0]
            ring.append(buf)

            if vad:
                is_speech = vad.is_speech(buf, vad_cfg.samplerate)
            else:
                is_speech = _rms_int16(buf) > (start_thr if not speaking else stop_thr)

            if not speaking:
                if is_speech:
                    speaking = True
                    # Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ
                    frames.extend(list(ring))
                    frames.append(buf)
                    silence_ms = 0
            else:
                frames.append(buf)
                if is_speech:
                    silence_ms = 0
                else:
                    silence_ms += vad_cfg.frame_ms
                    if silence_ms >= vad_cfg.max_silence_ms:
                        for _ in range(post_frames):
                            frames.append(stream.read(bytes_per_frame)[0])
                        break

    # Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ñ‹Ğ¹ WAV
    raw_pcm = b"".join(frames)
    bio = io.BytesIO()
    with wave.open(bio, "wb") as w:
        w.setnchannels(1)
        w.setsampwidth(2)
        w.setframerate(vad_cfg.samplerate)
        w.writeframes(raw_pcm)
    return bio.getvalue()


# ----------- STT / LLM -----------
def transcribe_bytes(client: OpenAI, cfg, wav_bytes: bytes) -> str:
    tmp = os.path.join(os.getcwd(), "hotword_input.wav")
    with open(tmp, "wb") as f:
        f.write(wav_bytes)
    try:
        with open(tmp, "rb") as f:
            resp = client.audio.transcriptions.create(
                model=cfg["STT_MODEL"],
                file=f,
                language=cfg["STT_LANGUAGE"] or "ru"
            )
        return getattr(resp, "text", "").strip()
    finally:
        try: os.remove(tmp)
        except: pass

def chat(client: OpenAI, cfg, user_text: str) -> str:
    resp = client.chat.completions.create(
        model=cfg["OPENAI_MODEL"],
        messages=[
            {"role": "system", "content": cfg["SYSTEM_PROMPT"]},
            {"role": "user", "content": user_text}
        ],
        temperature=0.3,
    )
    return resp.choices[0].message.content.strip()


# ----------- Wake Word -----------
def _normalize(text: str) -> str:
    # Ğ½Ğ¸Ğ¶Ğ½Ğ¸Ğ¹ Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€, Ñ‘â†’Ğµ, ÑƒĞ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ²ÑÑ‘ ĞºÑ€Ğ¾Ğ¼Ğµ Ğ±ÑƒĞºĞ²/Ñ†Ğ¸Ñ„Ñ€/Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ğ¾Ğ², ÑÑ…Ğ»Ğ¾Ğ¿Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹
    t = text.lower().replace("Ñ‘", "Ğµ")
    t = re.sub(r"[^a-zĞ°-Ñ0-9 ]+", " ", t)
    t = re.sub(r"\s+", " ", t).strip()
    return t

def contains_wake_word(text: str, wake_words: List[str]) -> bool:
    t = _normalize(text)
    # Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒĞµĞ¼ Ğ¸ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ÑŒ Ñ‚Ñ€Ğ¸Ğ³Ğ³ĞµÑ€Ğ¾Ğ²
    words_norm = [_normalize(w) for w in wake_words]
    return any(w in t for w in words_norm)


# ----------- OS-Bridge hook -----------
def maybe_execute_os_bridge(assistant_text: str) -> Tuple[str, Optional[str]]:
    os_line = None
    for line in assistant_text.splitlines():
        if line.strip().startswith("OS:"):
            os_line = line.strip()[3:].strip()
            break
    report = None
    if os_line:
        try:
            payload = json.loads(os_line)
            report = os_bridge.execute(payload)
            assistant_text = "\n".join(
                l for l in assistant_text.splitlines() if not l.strip().startswith("OS:")
            ).strip()
        except Exception as e:
            report = f"OS-Bridge: Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ñ€Ğ°Ğ·Ğ±Ğ¾Ñ€Ğ° ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹: {e}"
    return assistant_text, report


# ----------- Main -----------
def main():
    cfg = load_config()
    client = build_openai_client(cfg)

    print(rf"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Jarvis Hotword + VAD ({'webrtcvad' if USE_WEBRTCVAD else 'energy'}) + OS-Bridge   â”‚
â”‚         Ğ¡ĞºĞ°Ğ¶Ğ¸: "ĞŸÑ€Ğ¸Ğ²ĞµÑ‚, Ğ”Ğ¶Ğ°Ñ€Ğ²Ğ¸Ñ" / "Hey Jarvis"          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")
    print(f"OpenAI SDK: {openai_version}")
    print(f"LLM: {cfg['OPENAI_MODEL']} | STT: {cfg['STT_MODEL']} | TTS: {cfg['TTS_MODEL']}({cfg['TTS_VOICE']})")
    print(f"VAD: {'webrtcvad' if USE_WEBRTCVAD else 'energy'} (frame={cfg['VAD_FRAME_MS']} ms, "
          f"max_silence={cfg['VAD_MAX_SILENCE_MS']} ms, pre={cfg['VAD_PREROLL_MS']} ms, post={cfg['VAD_POSTROLL_MS']} ms)\n")

    if cfg["GREETING_ON_START"]:
        try: tts_say(client, cfg, cfg["GREETING_TEXT"])
        except Exception as e: print("ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ:", e)

    vad_cfg = VadConfig(
        samplerate=cfg["SAMPLE_RATE"],
        frame_ms=cfg["VAD_FRAME_MS"],
        max_silence_ms=cfg["VAD_MAX_SILENCE_MS"],
        aggressiveness=cfg["VAD_AGGRESSIVENESS"],
        preroll_ms=cfg["VAD_PREROLL_MS"],
        postroll_ms=cfg["VAD_POSTROLL_MS"],
    )

    while True:
        print("â³ Ğ–Ğ´Ñƒ wake-wordâ€¦ (Ctrl+C â€” Ğ²Ñ‹Ñ…Ğ¾Ğ´)")
        wav = record_utterance_vad(vad_cfg)
        text = transcribe_bytes(client, cfg, wav)
        if not text:
            continue
        print(f"ğŸ‘‚ Ğ£ÑĞ»Ñ‹ÑˆĞ°Ğ»: {text}")

        if cfg["WAKE_WORD_ENABLED"] and not contains_wake_word(text, cfg["WAKE_WORDS"]):
            continue

        if cfg["LISTENING_BEEP"]:
            winsound.MessageBeep()
        try: tts_say(client, cfg, "Ğ¡Ğ»ÑƒÑˆĞ°Ñ")
        except Exception: pass

        cmd_wav = record_utterance_vad(vad_cfg)
        user_text = transcribe_bytes(client, cfg, cmd_wav)
        if not user_text:
            tts_say(client, cfg, "ĞĞµ Ñ€Ğ°ÑÑĞ»Ñ‹ÑˆĞ°Ğ» ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñƒ.")
            continue

        print(f"ğŸ§‘ Ğ¢Ñ‹: {user_text}")
        print("ğŸ¤– Ğ”ÑƒĞ¼Ğ°Ñâ€¦")
        assistant = chat(client, cfg, user_text)
        assistant, os_report = maybe_execute_os_bridge(assistant)
        print(f"ğŸ¤– Jarvis: {assistant}")
        if os_report:
            print(f"ğŸ–¥  {os_report}")
            assistant += f"\n{os_report}"

        try: tts_say(client, cfg, assistant)
        except Exception as e: print("ĞÑˆĞ¸Ğ±ĞºĞ° TTS:", e)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nĞ’Ñ‹Ñ…Ğ¾Ğ´.")
